{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>940</td><td>application_1551670200070_0558</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://data-services3.cs.rutgers.edu:8088/proxy/application_1551670200070_0558/\">Link</a></td><td><a target=\"_blank\" href=\"http://data3.cs.rutgers.edu:8042/node/containerlogs/container_e68_1551670200070_0558_01_000001/hz333\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def weightRate(rate, time): #TODO: a better approach to weight rates using time\n",
    "\trate = float(rate)\n",
    "\treturn rate\n",
    "\n",
    "def weightProfi(rate, profi): #TODO: a better approach to weight profiles using rate\n",
    "\tif rate <= 0:\n",
    "\t\treturn [0 for profiDim in profi]\n",
    "\telse:\n",
    "\t\treturn [rate * profiDim for profiDim in profi]\n",
    "\n",
    "def getMovieLine(row):\n",
    "\tmIdx = row[0]\n",
    "\tprofi = row[1:]\n",
    "\treturn (mIdx, [float(profiDim) for profiDim in profi])\n",
    "\n",
    "def getRateLine(row):\n",
    "\tuIdx, mIdx, rate, time = row\n",
    "\twRate = weightRate(rate, time)\n",
    "\treturn (uIdx, (mIdx, wRate))\n",
    "\n",
    "def getUserRate(line):\n",
    "\tuIdx, mRate = line\n",
    "\tmIdx, rate = mRate\n",
    "\treturn (uIdx, (1, rate, rate ** 2))\n",
    "\n",
    "def sumUserRate(x, y):\n",
    "\txC, xR, xR2 = x\n",
    "\tyC, yR, yR2 = y\n",
    "\treturn (xC + yC, xR + yR, xR2 + yR2)\n",
    "\n",
    "class rateNormalizer(object):\n",
    "\tdef __init__(self, userRateCount):\n",
    "\t\tself.meanRate = dict()\n",
    "\t\tself.varRate = dict()\n",
    "\t\tfor uIdx, rateCount in userRateCount:\n",
    "\t\t\tcount, rate, rate2 = rateCount\n",
    "\t\t\tself.meanRate[uIdx] = rate / count\n",
    "\t\t\tself.varRate[uIdx] = (rate2 / count) - self.meanRate[uIdx] ** 2\n",
    "\t\tself.sqrtMeanVarRate = math.sqrt(sum(self.varRate.values()) / len(self.varRate))\n",
    "\t\treturn\n",
    "\n",
    "\tdef normalize(self, line):\n",
    "\t\tuIdx, mRate = line\n",
    "\t\tmIdx, rate = mRate\n",
    "\t\t#assume the rate is normal dist., normalize to N(0, 1)\n",
    "\t\tnRate = 0.1 + (rate - self.meanRate[uIdx]) / (math.sqrt(self.varRate[uIdx]) + self.sqrtMeanVarRate) #TODO: a better approach to avoid dividing by 0\n",
    "\t\treturn (mIdx, (uIdx, nRate))\n",
    "\n",
    "def getUserComponent(data):\n",
    "\tmIdx, UM = data\n",
    "\tmProfi, uRate = UM\n",
    "\tuIdx, rate = uRate\n",
    "\tuProfiCompo = weightProfi(rate, mProfi)\n",
    "\treturn (uIdx, uProfiCompo)\n",
    "\n",
    "def sumUserProfi(x, y):\n",
    "\treturn [x[i] + y[i] for i in range(len(x))]\n",
    "\n",
    "def formatCSV(line):\n",
    "\tuIdx, uProfi = line\n",
    "\tres = [uIdx]\n",
    "\tfor uProfiDim in uProfi:\n",
    "\t\tres.append(uProfiDim)\n",
    "\treturn res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieData = spark.read.csv('/user/hz333/data/project/movieMetaProfile.csv', header = False)\n",
    "rateData = spark.read.csv('/user/hz333/data/project/ratings.csv', header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(mIdx, mProfi) => (mIdx, [mProfi])\n",
    "movie = movieData.rdd.map(getMovieLine)\n",
    "#(uIdx, mIdx, rate, time) => (uIdx, (mIdx, wRate))\n",
    "rate = rateData.rdd.map(getRateLine)\n",
    "\n",
    "#(uIdx, (mIdx, rate)) => (uIdx, (1, rate, rate2))\n",
    "userRate = rate.map(getUserRate)\n",
    "#(uIdx, (1, rate, rate)) => (uIdx, (count, sumRate, sumRate2))\n",
    "userRate = userRate.reduceByKey(sumUserRate)\n",
    "\n",
    "userRate = userRate.collect()\n",
    "rateNorm = rateNormalizer(userRate)\n",
    "\n",
    "#(uIdx, (mIdx, rate)) => (mIdx, (uIdx, nRate))\n",
    "MURate = rate.map(rateNorm.normalize)\n",
    "#(mIdx, mProfi), (mIdx, (uIdx, rate)) => (mIdx, (mProfi, (uIdx, rate)))\n",
    "MUM = movie.join(MURate)\n",
    "\n",
    "#(mIdx, (mProfi, (uIdx, rate))) => (uIdx, [uProfiCompo])\n",
    "uProfiCompo = MUM.map(getUserComponent)\n",
    "#(uIdx, [uProfiCompo]) => (uIdx, [uProfi])\n",
    "uProfi = uProfiCompo.reduceByKey(sumUserProfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', [0.0, 80.47164437342879, 94.51310751218831, 57.932530343214594, 72.02264824491319, 49.4118315957792, 51.32307933598421, 0.0, 50.00890242593793, 54.00985944005231, 4.064035812205046, 6.864016828072797, 0.0, 52.30238975995697, 29.48471919760489, 19.285233305525182, 38.870603420170355, 45.07166220323994, 38.71461823793791, 10.756180244835514])]"
     ]
    }
   ],
   "source": [
    "uProfi.filter(lambda x: x[0] == '1').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610"
     ]
    }
   ],
   "source": [
    "len(userRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(uIdx, [uProfi]) => (uIdx, uProfi)\n",
    "uProfiCSV = uProfi.map(formatCSV)\n",
    "uProfiCSV = spark.createDataFrame(uProfiCSV, samplingRatio = 1)\n",
    "uProfiCSV.write.option('header', 'false').csv('/user/hz333/data/project/uMetaProfi.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
